package com.example.arspapp_ui

import android.Manifest
import android.annotation.SuppressLint
import android.app.AlertDialog
import android.app.Dialog
import android.content.Context
import android.content.Intent
import android.content.SharedPreferences
import android.content.pm.PackageManager
import android.graphics.*
import android.hardware.Sensor
import android.hardware.SensorManager
import android.hardware.camera2.*
import android.hardware.camera2.CameraDevice.StateCallback
import android.hardware.camera2.CameraDevice.TEMPLATE_PREVIEW
import android.media.CamcorderProfile
import android.media.Image
import android.media.ImageReader
import android.media.ImageReader.OnImageAvailableListener
import android.media.MediaRecorder
import android.net.Uri
import android.os.*
import android.util.Log
import android.util.Size
import android.util.SparseIntArray
import android.view.*
import android.widget.Toast
import androidx.annotation.RequiresApi
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import androidx.fragment.app.DialogFragment
import androidx.fragment.app.Fragment
import androidx.fragment.app.FragmentManager
import org.tensorflow.lite.examples.posenet.lib.BodyPart
import org.tensorflow.lite.examples.posenet.lib.Person
import org.tensorflow.lite.examples.posenet.lib.Posenet
import java.io.File
import java.io.IOException
import java.text.SimpleDateFormat
import java.util.*
import java.util.concurrent.Semaphore
import java.util.concurrent.TimeUnit
import kotlin.collections.ArrayList
import kotlin.math.abs

class quick_posenet :
        Fragment(),
        ActivityCompat.OnRequestPermissionsResultCallback {


    /** List of body joints that should be connected.    */
    private val bodyJoints = listOf(
            Pair(BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP),
            Pair(BodyPart.LEFT_HIP, BodyPart.LEFT_KNEE),
            Pair(BodyPart.LEFT_KNEE, BodyPart.LEFT_ANKLE),
            Pair(BodyPart.RIGHT_HIP, BodyPart.RIGHT_KNEE),
            Pair(BodyPart.RIGHT_KNEE, BodyPart.RIGHT_ANKLE)
    )

    private var mNextVideoAbsolutePath: String? = null
    private val DETAIL_PATH = "DCIM/test1/"
    private var mediaRecorder: MediaRecorder? = null
    var Cachedir: File?=null
    /** Threshold for confidence score. */
    private val minConfidence = 0.5

    /** Radius of circle used to draw keypoints.  */
    private val circleRadius = 8.0f

    /** Paint class holds the style and color information to draw geometries,text and bitmaps. */
    private var paint = Paint()

    /** A shape for extracting frame data.   */
    private val PREVIEW_WIDTH = 640
    private val PREVIEW_HEIGHT = 480

    /** An object for the Posenet library.    */
    private lateinit var posenet: Posenet

    /** ID of the current [CameraDevice].   */
    private var cameraId: String? = null

    /** A [SurfaceView] for camera preview.   */
    private var surfaceView: SurfaceView? = null

    /** A [CameraCaptureSession] for camera preview.   */
    private var captureSession: CameraCaptureSession? = null

    /** A [CameraCaptureSession] for camera preview.   */
    private var previewSession: CameraCaptureSession? = null

    /** A reference to the opened [CameraDevice].    */
    private var cameraDevice: CameraDevice? = null

    /** The [android.util.Size] of camera preview.  */
    private var previewSize: Size? = null

    /** The [android.util.Size.getWidth] of camera preview. */
    private var previewWidth = 0

    /** The [android.util.Size.getHeight] of camera preview.  */
    private var previewHeight = 0

    /** A counter to keep count of total frames.  */
    private var frameCounter = 0

    private var saveframe=-100

    /** An IntArray to save image data in ARGB8888 format  */
    private lateinit var rgbBytes: IntArray

    /** A ByteArray to save image data in YUV format  */
    private var yuvBytes = arrayOfNulls<ByteArray>(3)

    /** An additional thread for running tasks that shouldn't block the UI.   */
    private var backgroundThread: HandlerThread? = null

    /** A [Handler] for running tasks in the background.    */
    private var backgroundHandler: Handler? = null

    /** An [ImageReader] that handles preview frame capture.   */
    private var imageReader: ImageReader? = null

    /** [CaptureRequest.Builder] for the camera preview   */
    private var previewRequestBuilder: CaptureRequest.Builder? = null

    /** [CaptureRequest] generated by [.previewRequestBuilder   */
    private var previewRequest: CaptureRequest? = null

    /** A [Semaphore] to prevent the app from exiting before closing the camera.    */
    private val cameraOpenCloseLock = Semaphore(1)

    /** Whether the current camera device supports Flash or not.    */
    private var flashSupported = false

    /** Orientation of the camera sensor.   */
    private var sensorOrientation: Int? = null

    /** Abstract interface to someone holding a display surface.    */
    private var surfaceHolder: SurfaceHolder? = null

    private var recordingSurface: Surface?=null

    private var mRecorderSurface: Surface?=null

    private var deviceOrientation: DeviceOrientation? = null

    private lateinit var mSensorManager: SensorManager

    private lateinit var mAccelerometer: Sensor
    private lateinit var mMagnetometer: Sensor
    var left_ankle:Point= Point(-1,-1)
    var right_ankle:Point=Point(-1,-1)
    var nose:Point=Point(-1,-1)
    var left_knee:Point=Point(-1,-1)
    var right_knee:Point?=Point(-1,-1)
    var Result_Boundary_Check= 0

    // var tracking = com.example.arspapp_ui.tracking()

    var setting_time:Int?=null
    var key_list=java.util.ArrayList<Point>()
    var start_joint_list=java.util.ArrayList<Point>()
    var stop_joint_list=java.util.ArrayList<Point>()
    var startPoint:Point?=null
    var stopPoint:Point?=null
    var angle_sig= 0
    var shoot_check=false
    var min=0
    var grade=0
    var left_check=false
    /** [CameraDevice.StateCallback] is called when [CameraDevice] changes its state.   */
    private val stateCallback = object : StateCallback() {

        override fun onOpened(cameraDevice: CameraDevice) {
            cameraOpenCloseLock.release()
            this@quick_posenet.cameraDevice = cameraDevice
            createCameraPreviewSession()
        }

        override fun onDisconnected(cameraDevice: CameraDevice) {
            cameraOpenCloseLock.release()
            cameraDevice.close()
            this@quick_posenet.cameraDevice = null
        }

        override fun onError(cameraDevice: CameraDevice, error: Int) {
            onDisconnected(cameraDevice)
            this@quick_posenet.activity?.finish()
        }
    }

    /**
     * A [CameraCaptureSession.CaptureCallback] that handles events related to JPEG capture.
     */
    private val captureCallback = object : CameraCaptureSession.CaptureCallback() {
        override fun onCaptureProgressed(
                session: CameraCaptureSession,
                request: CaptureRequest,
                partialResult: CaptureResult
        ) {
        }

        override fun onCaptureCompleted(
                session: CameraCaptureSession,
                request: CaptureRequest,
                result: TotalCaptureResult
        ) {
        }
    }

    /**
     * Shows a [Toast] on the UI thread.
     *
     * @param text The message to show
     */
    private fun showToast(text: String) {
        val activity = activity
        activity?.runOnUiThread { Toast.makeText(activity, text, Toast.LENGTH_SHORT).show() }
    }

    override fun onCreateView(
            inflater: LayoutInflater,
            container: ViewGroup?,
            savedInstanceState: Bundle?
    ): View? = inflater.inflate(R.layout.activity_posenet, container, false)






    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {

        surfaceView = view.findViewById(R.id.surfaceView)
        mSensorManager = requireContext().getSystemService(Context.SENSOR_SERVICE) as SensorManager
        mAccelerometer = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER)
        mMagnetometer = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD)
        initSensor()
        deviceOrientation = DeviceOrientation()
        surfaceHolder = surfaceView!!.holder

        Cachedir=requireContext()!!.cacheDir
        var settings:SharedPreferences = requireContext().getSharedPreferences("pref",0)
        setting_time =settings.getInt("secends",0)
        if(setting_time==30) min=139
        else if(setting_time==40) min==185
        else if(setting_time==50) min=231
        else min=277
    }

    override fun onResume() {
        super.onResume()
        startBackgroundThread()
        mSensorManager.registerListener(
                deviceOrientation!!.eventListener, mAccelerometer, SensorManager.SENSOR_DELAY_UI
        )
        mSensorManager.registerListener(
                deviceOrientation!!.eventListener, mMagnetometer, SensorManager.SENSOR_DELAY_UI
        )
    }

    override fun onStart() {
        super.onStart()
        openCamera()

        posenet = Posenet(this.requireContext())
    }

    override fun onPause() {
        super.onPause()
        closeCamera()
        stopBackgroundThread()
        mSensorManager.unregisterListener(deviceOrientation!!.eventListener)
        onDestroy()
    }

    override fun onDestroy() {
        super.onDestroy()
        posenet.close()

    }

    private fun requestCameraPermission() {
        if (shouldShowRequestPermissionRationale(Manifest.permission.CAMERA)) {
            ConfirmationDialog().show(childFragmentManager, FRAGMENT_DIALOG)
        } else {
            requestPermissions(arrayOf(Manifest.permission.CAMERA), REQUEST_CAMERA_PERMISSION)
        }
    }

    override fun onRequestPermissionsResult(
            requestCode: Int,
            permissions: Array<String>,
            grantResults: IntArray
    ) {

        if (requestCode == REQUEST_CAMERA_PERMISSION) {
            if (allPermissionsGranted(grantResults)) {
                ErrorDialog.newInstance(getString(R.string.tfe_pn_request_permission))
                        .show(childFragmentManager, FRAGMENT_DIALOG)
            }
        } else {
            super.onRequestPermissionsResult(requestCode, permissions, grantResults)
        }
    }

    private fun allPermissionsGranted(grantResults: IntArray) = grantResults.all {
        it == PackageManager.PERMISSION_GRANTED
    }

    private fun initSensor() {
        mSensorManager = this.requireActivity().getSystemService(Context.SENSOR_SERVICE) as SensorManager
        mAccelerometer = mSensorManager.getDefaultSensor(Sensor.TYPE_ACCELEROMETER)
        mMagnetometer = mSensorManager.getDefaultSensor(Sensor.TYPE_MAGNETIC_FIELD)
    }

    /**
     * Sets up member variables related to camera.
     */
    private fun setUpCameraOutputs() {

        val activity = activity
        val manager = this.requireActivity().getSystemService(Context.CAMERA_SERVICE) as CameraManager
        try {
            for (cameraId in manager.cameraIdList) {
                val characteristics = manager.getCameraCharacteristics(cameraId)

                // We don't use a front facing camera in this sample.
                val cameraDirection = characteristics.get(CameraCharacteristics.LENS_FACING)
                if (cameraDirection != null &&
                        cameraDirection == CameraCharacteristics.LENS_FACING_FRONT
                ) {
                    continue
                }

                previewSize = Size(PREVIEW_WIDTH, PREVIEW_HEIGHT)

                imageReader = ImageReader.newInstance(
                        PREVIEW_WIDTH, PREVIEW_HEIGHT,
                        ImageFormat.YUV_420_888, /*maxImages*/ 20
                )

                sensorOrientation = characteristics.get(CameraCharacteristics.SENSOR_ORIENTATION)!!

                previewHeight = previewSize!!.height
                previewWidth = previewSize!!.width

                // Initialize the storage bitmaps once when the resolution is known.
                rgbBytes = IntArray(previewWidth * previewHeight)

                // Check if the flash is supported.
                flashSupported =
                        characteristics.get(CameraCharacteristics.FLASH_INFO_AVAILABLE) == true

                this.cameraId = cameraId

                // We've found a viable camera and finished setting up member variables,
                // so we don't need to iterate through other available cameras.
                return
            }
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        } catch (e: NullPointerException) {
            // Currently an NPE is thrown when the Camera2API is used but not supported on the
            // device this code runs.
            ErrorDialog.newInstance(getString(R.string.tfe_pn_camera_error))
                    .show(childFragmentManager, FRAGMENT_DIALOG)
        }
    }

    /**
     * Opens the camera specified by [PosenetActivity.cameraId].
     */
    private fun openCamera() {
        val permissionCamera =
                ContextCompat.checkSelfPermission(requireActivity(), Manifest.permission.CAMERA)
        if (permissionCamera != PackageManager.PERMISSION_GRANTED) {
            requestCameraPermission()
        }
        setUpCameraOutputs()
        val manager = requireActivity().getSystemService(Context.CAMERA_SERVICE) as CameraManager
        try {
            // Wait for camera to open - 2.5 seconds is sufficient
            if (!cameraOpenCloseLock.tryAcquire(2500, TimeUnit.MILLISECONDS)) {
                throw RuntimeException("Time out waiting to lock camera opening.")
            }
            manager.openCamera(cameraId!!, stateCallback, backgroundHandler)
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        } catch (e: InterruptedException) {
            throw RuntimeException("Interrupted while trying to lock camera opening.", e)
        }
    }

    /**
     * Closes the current [CameraDevice].
     */
    fun closeCamera() {
        if (captureSession == null) {
            return
        }

        try {
            cameraOpenCloseLock.acquire()
            captureSession!!.close()
            captureSession = null
            cameraDevice!!.close()
            cameraDevice = null
            imageReader!!.close()
            imageReader = null
        } catch (e: InterruptedException) {
            throw RuntimeException("Interrupted while trying to lock camera closing.", e)
        } finally {
            cameraOpenCloseLock.release()
        }


    }

    /**
     * Starts a background thread and its [Handler].
     */
    private fun startBackgroundThread() {
        backgroundThread = HandlerThread("imageAvailableListener").also { it.start() }
        backgroundHandler = Handler(backgroundThread!!.looper)
        Log.i(TAG, "threading start")
    }

    /**
     * Stops the background thread and its [Handler].
     */
    private fun stopBackgroundThread() {
        backgroundThread?.quitSafely()
        try {
            backgroundThread?.join()
            backgroundThread = null
            backgroundHandler = null
        } catch (e: InterruptedException) {
            Log.e(TAG, e.toString())
        }
    }

    /** Fill the yuvBytes with data from image planes.   */
    private fun fillBytes(planes: Array<Image.Plane>, yuvBytes: Array<ByteArray?>) {
        // Row stride is the total number of bytes occupied in memory by a row of an image.
        // Because of the variable row stride it's not possible to know in
        // advance the actual necessary dimensions of the yuv planes.
        for (i in planes.indices) {
            val buffer = planes[i].buffer
            if (yuvBytes[i] == null) {
                yuvBytes[i] = ByteArray(buffer.capacity())
            }
            buffer.get(yuvBytes[i]!!)
        }
    }

    /** A [OnImageAvailableListener] to receive frames as they are available.  */
    private var imageAvailableListener = object : OnImageAvailableListener {
        @RequiresApi(Build.VERSION_CODES.N)
        override fun onImageAvailable(imageReader: ImageReader) {
            // We need wait until we have some size from onPreviewSizeChosen
            if (previewWidth == 0 || previewHeight == 0) {
                return
            }

            val image = imageReader.acquireLatestImage() ?: return
            fillBytes(image.planes, yuvBytes)

            ImageUtils.convertYUV420ToARGB8888(
                    yuvBytes[0]!!,
                    yuvBytes[1]!!,
                    yuvBytes[2]!!,
                    previewWidth,
                    previewHeight,
                    /*yRowStride=*/ image.planes[0].rowStride,
                    /*uvRowStride=*/ image.planes[1].rowStride,
                    /*uvPixelStride=*/ image.planes[1].pixelStride,
                    rgbBytes
            )

            // Create bitmap from int array
            val imageBitmap = Bitmap.createBitmap(
                    rgbBytes, previewWidth, previewHeight,
                    Bitmap.Config.ARGB_8888
            )

            // Create rotated version for portrait display
            val rotateMatrix = Matrix()
            rotateMatrix.postRotate(0.0f)

            val rotatedBitmap = Bitmap.createBitmap(
                    imageBitmap, 0, 0, previewWidth, previewHeight,
                    rotateMatrix, true
            )
            image.close()
            // Process an image for analysis in every 3 frames.


            processImage(imageBitmap)

        }
    }

    /** Crop Bitmap to maintain aspect ratio of model input.   */
    private fun cropBitmap(bitmap: Bitmap): Bitmap {
        val bitmapRatio = bitmap.height.toFloat() / bitmap.width
        val modelInputRatio = MODEL_HEIGHT.toFloat() / MODEL_WIDTH
        var croppedBitmap = bitmap

        // Acceptable difference between the modelInputRatio and bitmapRatio to skip cropping.
        val maxDifference = 1e-5

        // Checks if the bitmap has similar aspect ratio as the required model input.
        when {
            abs(modelInputRatio - bitmapRatio) < maxDifference -> return croppedBitmap
            modelInputRatio < bitmapRatio -> {
                // New image is taller so we are height constrained.
                val cropHeight = bitmap.height - (bitmap.width.toFloat() / modelInputRatio)
                croppedBitmap = Bitmap.createBitmap(
                        bitmap,
                        0,
                        (cropHeight / 2).toInt(),
                        bitmap.width,
                        (bitmap.height - cropHeight).toInt()
                )
            }
            else -> {
                val cropWidth = bitmap.width - (bitmap.height.toFloat() * modelInputRatio)
                croppedBitmap = Bitmap.createBitmap(
                        bitmap,
                        (cropWidth / 2).toInt(),
                        0,
                        (bitmap.width - cropWidth).toInt(),
                        bitmap.height
                )
            }
        }
        return croppedBitmap
    }

    /** Set the paint color and size.    */
    private fun setPaint() {
        paint.color = Color.RED
        paint.textSize = 80.0f
        paint.strokeWidth = 8.0f
    }

    private fun setPaint2() {
        paint.color = Color.BLUE
        paint.textSize = 80.0f
        paint.strokeWidth = 8.0f
    }
    private fun setPaint3() {
        paint.color = Color.WHITE
        paint.textSize = 80.0f
        paint.strokeWidth = 15.0f
    }
    private fun setPaint4() {
        paint.color = Color.GREEN
        paint.textSize = 80.0f
        paint.strokeWidth = 15.0f
    }
    private fun setPainttxt() {
        paint.color = Color.RED
        paint.textSize = 120.0f
        paint.strokeWidth = 8.0f
    }

    private fun setPaint2txt() {
        paint.color = Color.BLUE
        paint.textSize = 120.0f
        paint.strokeWidth = 8.0f
    }
    /** Draw bitmap on Canvas.   */
    @RequiresApi(Build.VERSION_CODES.N)
    private fun draw(canvas: Canvas, person: Person, bitmap: Bitmap) {


        if(frameCounter==min){

            stopRecording(true)

            mNextVideoAbsolutePath!!.let { transfer_intant(it) }
        }
        Log.i("count",frameCounter.toString())

        canvas.drawColor(Color.TRANSPARENT, PorterDuff.Mode.CLEAR)
        // Draw `bitmap` and `person` in square canvas.
        val screenWidth: Int
        val screenHeight: Int
        val left: Int
        val right: Int
        val top: Int
        val bottom: Int
        if (canvas.height > canvas.width) {
            screenWidth = canvas.width
            screenHeight = canvas.width
            left = 0
            top = (canvas.height - canvas.width) / 2
        } else {
            screenWidth = canvas.height
            screenHeight = canvas.height
            left = (canvas.width - canvas.height) / 2
            top = 0
        }
        right = left + screenWidth
        bottom = top + screenHeight

        setPaint()
        canvas.drawBitmap(
                bitmap,
                Rect(0, 0, bitmap.width, bitmap.height),
                Rect(left, top, right, bottom),
                paint
        )

        val widthRatio = screenWidth.toFloat() / MODEL_WIDTH
        val heightRatio = screenHeight.toFloat() / MODEL_HEIGHT
        var footkey=0
        var footflag=0
        // Draw key points over the image.
        for (keyPoint in person.keyPoints) {

            if (keyPoint.score > minConfidence) {
                val position = keyPoint.position


                if(footkey==15) {
                    left_ankle=Point(position.x,position.y)
                    //if(position.x!=0) Log.i("관절", "왼쪽발목 :"+left_ankle.toString())
                }
                if(footkey==16) {
                    right_ankle=Point(position.x,position.y)
                    // if(position.x!=0) Log.i("관절", "오른쪽발목 :"+right_ankle.toString())
                }


                val adjustedX: Float = position.x.toFloat() * widthRatio + left
                val adjustedY: Float = position.y.toFloat() * heightRatio + top
                if(frameCounter==saveframe+1&&angle_sig==0){
                    angle_sig=1
                    key_list!!.add(footflag, Point(position.x,position.y))
                    //Log.i("원",key_list.get(footkey).toString())
                    footflag++
                }

                canvas.drawCircle(adjustedX, adjustedY, circleRadius, paint)
            }
            footkey++
        }

        for (line in bodyJoints) {

            if (
                    (person.keyPoints[line.first.ordinal].score > minConfidence) and
                    (person.keyPoints[line.second.ordinal].score > minConfidence)
            ) {
                canvas.drawLine(
                        person.keyPoints[line.first.ordinal].position.x.toFloat() * widthRatio + left,
                        person.keyPoints[line.first.ordinal].position.y.toFloat() * heightRatio + top,
                        person.keyPoints[line.second.ordinal].position.x.toFloat() * widthRatio + left,
                        person.keyPoints[line.second.ordinal].position.y.toFloat() * heightRatio + top,
                        paint
                )
            }
        }
        var resource=requireContext().resources
        var blood = BitmapFactory.decodeResource(resource, R.drawable.blood)
        var check=0

        if(left_ankle!=null&&left_ankle!!.x!=null) {
            check = BoundaryCheck(left_ankle,left_check)
            if (check ==1) {
                setPaint2txt()
                canvas.drawText(
                        "+5",
                        (200.0f),
                        (200.0f),
                        paint
                )
                canvas.drawText(
                        "굿!!",
                        (150.0f),
                        (380.0f),
                        paint
                )
            }
            else if (check ==3) {
                canvas.drawBitmap(blood,620.0f,5.0f,paint)
                setPainttxt()
                canvas.drawText(
                        "-3",
                        (200.0f),
                        (200.0f),
                        paint
                )
                canvas.drawText(
                        "가시 조심!!",
                        (100.0f),
                        (380.0f),
                        paint
                )
            }
        }
        check = 0
        if(right_ankle!=null&&right_ankle!!.x!=null) {
            check = BoundaryCheck(right_ankle,left_check)
            if (check ==1) {
                setPaint2txt()
                canvas.drawText(
                        "+5",
                        (200.0f),
                        (200.0f),
                        paint
                )
                canvas.drawText(
                        "굿!!",
                        (150.0f),
                        (380.0f),
                        paint
                )
            }
            else if (check ==3) {
                canvas.drawBitmap(blood,620.0f,50.0f,paint)
                setPainttxt()
                canvas.drawText(
                        "-3",
                        (200.0f),
                        (200.0f),
                        paint
                )
                canvas.drawText(
                        "가시 조심!!",
                        (100.0f),
                        (380.0f),
                        paint
                )
            }
        }
        check = 0
        var ball = BitmapFactory.decodeResource(resource, R.drawable.ball)
        canvas.drawBitmap(ball,750.0f,800.0f,paint)
        canvas.drawBitmap(ball,14500.0f,800.0f,paint)

        /* setPaint4()
         canvas.drawLine(900.0f,750.0f,550.0f,750.0f,paint) //가로
         canvas.drawLine(900.0f,750.0f,900.0f,1000.0f,paint)
         canvas.drawLine(1250.0f,750.0f,1600.0f,750.0f,paint)  //세로
         canvas.drawLine(1250.0f,750.0f,1250.0f,1000.0f,paint)
         setPaint()
         canvas.drawLine(900.0f,750.0f,1250.0f,750.0f,paint)
         setPaint2()*/

        setPaint3()
        canvas.drawText(
                "점수: %d".format(grade),
                (15.0f * widthRatio+right),
                (30.0f * heightRatio),
                paint
        )
        canvas.drawText(
                "사람 인식 속도: %.2f ms".format(posenet.lastInferenceTimeNanos * 1.0f / 1_000_000),
                (15.0f * widthRatio+right),
                (50.0f * heightRatio ),
                paint
        )

        setPaint3()


        frameCounter++


        // Draw!
        surfaceHolder!!.unlockCanvasAndPost(canvas)

    }


    /** Process image using Posenet library.   */
    @RequiresApi(Build.VERSION_CODES.N)
    private fun processImage(bitmap: Bitmap) {
        // Crop bitmap.

        val croppedBitmap = cropBitmap(bitmap)

        // Created scaled version of bitmap for model input.
        val scaledBitmap = Bitmap.createScaledBitmap(croppedBitmap, MODEL_WIDTH, MODEL_HEIGHT, true)

        // Perform inference.
        val person = posenet.estimateSinglePose(scaledBitmap)
        val canvas: Canvas = surfaceHolder!!.lockCanvas()
        draw(canvas, person, scaledBitmap)

    }

    /**
     * Creates a new [CameraCaptureSession] for camera preview.
     */
    private fun createCameraPreviewSession() {
        try {

            if (mediaRecorder == null) {
                mediaRecorder = MediaRecorder()
            }

            mediaRecorder!!.setVideoEncodingBitRate(10000000)
            //mediaRecorder!!.setMaxDuration(10000) // 10 seconds
            mediaRecorder!!.setVideoSource(MediaRecorder.VideoSource.SURFACE)

            mediaRecorder!!.setOutputFormat(MediaRecorder.OutputFormat.MPEG_4)
            mediaRecorder!!.setVideoFrameRate(20)

            val camcorderProfile = CamcorderProfile.get(CamcorderProfile.QUALITY_HIGH)

            if (camcorderProfile.videoFrameWidth > previewSize!!.width
                    || camcorderProfile.videoFrameHeight > previewSize!!.height
            ) {
                camcorderProfile.videoFrameWidth = previewSize!!.width
                camcorderProfile.videoFrameHeight = previewSize!!.height
            }

            mediaRecorder!!.setVideoSize(
                    camcorderProfile.videoFrameWidth,
                    camcorderProfile.videoFrameHeight
            )

            mediaRecorder!!.setVideoEncoder(MediaRecorder.VideoEncoder.H264)


            //mediaRecorder!!.setOrientationHint(90)
            if (mNextVideoAbsolutePath == null || mNextVideoAbsolutePath!!.isEmpty()) {
                mNextVideoAbsolutePath = getVideoFilePath()
            }
            mediaRecorder!!.setOutputFile(mNextVideoAbsolutePath)

            try {
                mediaRecorder!!.prepare()
            } catch (e: IOException) {
                e.printStackTrace()
                return
            }

            val surfaces: MutableList<Surface> = ArrayList()
            mRecorderSurface = mediaRecorder!!.surface
            // We capture images from preview in YUV format.
            imageReader = ImageReader.newInstance(
                    previewSize!!.width, previewSize!!.height, ImageFormat.YUV_420_888, 20
            )
            imageReader!!.setOnImageAvailableListener(imageAvailableListener, backgroundHandler)

            // This is the surface we need to record images for processing.
            recordingSurface = imageReader!!.surface

            // We set up a CaptureRequest.Builder with the output Surface.
            previewRequestBuilder = cameraDevice!!.createCaptureRequest(
                    TEMPLATE_PREVIEW
            )
            surfaces.add(recordingSurface!!)

            previewRequestBuilder!!.addTarget(recordingSurface)
            Log.i(TAG,"start")
            surfaces.add(mRecorderSurface!!)

            previewRequestBuilder!!.addTarget(mRecorderSurface)



            try {

                cameraDevice!!.createCaptureSession(
                        surfaces,
                        object : CameraCaptureSession.StateCallback() {
                            override fun onConfigured(cameraCaptureSession: CameraCaptureSession) {
                                // The camera is already closed
                                if (cameraDevice == null) return

                                // When the session is ready, we start displaying the preview.
                                captureSession = cameraCaptureSession
                                try {
                                    // Auto focus should be continuous for camera preview.
                                    previewRequestBuilder!!.set(
                                            CaptureRequest.CONTROL_AF_MODE,
                                            CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE
                                    )
                                    // Flash is automatically enabled when necessary.
                                    setAutoFlash(previewRequestBuilder!!)
                                    // Finally, we start displaying the camera preview.
                                    previewRequest = previewRequestBuilder!!.build()
                                    captureSession!!.setRepeatingRequest(
                                            previewRequest!!,
                                            null, null
                                    )

                                } catch (e: CameraAccessException) {
                                    Log.e(TAG, e.toString())
                                }
                            }

                            override fun onConfigureFailed(cameraCaptureSession: CameraCaptureSession) {
                                showToast("Failed")
                            }
                        },
                        null
                )
                mediaRecorder!!.start()

            } catch (e: CameraAccessException) {
                e.printStackTrace()
            }
            // Here, we create a CameraCaptureSession for camera preview.

        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }
    }

    private fun setAutoFlash(requestBuilder: CaptureRequest.Builder) {
        if (flashSupported) {
            requestBuilder.set(
                    CaptureRequest.CONTROL_AE_MODE,
                    CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH
            )
        }
    }




    private fun getVideoFilePath(): String? {
        val dir = Environment.getExternalStorageDirectory().absoluteFile
        val time = System.currentTimeMillis() //시간 받기

        val sdf: SimpleDateFormat = SimpleDateFormat("yyyy-MM-dd HH.mm.ss")
        //포멧 변환  형식 만들기
        //포멧 변환  형식 만들기
        val dd = Date(time) //받은 시간을 Date 형식으로 바꾸기

        val strTime: String = sdf.format(dd) //Data 정보를 포멧 변환하기

        val path =
                dir.path + "/" + DETAIL_PATH
        val dst = File(path)
        if (!dst.exists()) dst.mkdirs()
        return path + strTime + ".mp4"
    }

    private fun stopRecording(showPreview: Boolean) {
        Log.i(TAG,"?щ줈2")


        if(showPreview){

            val file = File(mNextVideoAbsolutePath)

            if(!file.exists()){
                try {
                    Log.i("file ?놁쓬", "?뚯씪 ?곸쐞 ?붾젆?좊━ ?앹꽦")
                    file.mkdirs()
                } catch (e:Exception){
                    Log.e("path.mkdirs", e.toString())
                }
            }
            mediaRecorder!!.stop()
            mediaRecorder!!.reset()
            mediaRecorder!!.release()
            previewRequestBuilder=null
            mRecorderSurface!!.release()
            mRecorderSurface=null
            recordingSurface!!.release()
            recordingSurface=null
            previewRequest=null
            requireContext()!!.sendBroadcast(
                    Intent(
                            Intent(
                                    Intent.ACTION_MEDIA_SCANNER_SCAN_FILE,
                                    Uri.fromFile(file)
                            )
                    )
            )
            requireContext()!!.sendBroadcast(
                    Intent(
                            Intent.ACTION_MEDIA_SCANNER_SCAN_FILE,
                            Uri.fromFile(file)
                    )
            )
            requireActivity().getApplicationContext().sendBroadcast(
                    Intent(
                            Intent.ACTION_MEDIA_SCANNER_SCAN_FILE,
                            Uri.fromFile(file)
                    )
            )
            this@quick_posenet.activity?.sendBroadcast(
                    Intent(
                            Intent.ACTION_MEDIA_SCANNER_SCAN_FILE,
                            Uri.fromFile(file)
                    )
            )
            this@quick_posenet.activity?.sendBroadcast(
                    Intent(
                            Intent(
                                    Intent.ACTION_MEDIA_SCANNER_SCAN_FILE,
                                    Uri.fromFile(file)
                            )
                    )
            )
            requireActivity().getApplicationContext().sendBroadcast( Intent(Intent.ACTION_MEDIA_SCANNER_SCAN_FILE, Uri.fromFile(file)))

            closeCamera()
        }
    }

    private val captureStateCallback: CameraCaptureSession.StateCallback =
            object : CameraCaptureSession.StateCallback() {
                override fun onConfigured(session: CameraCaptureSession) {
                    previewSession = session

                }
                override fun onConfigureFailed(session: CameraCaptureSession) {
                    showToast("Failed")
                }

            }




    /**
     * Shows an error message dialog.
     */
    class ErrorDialog : DialogFragment() {

        override fun onCreateDialog(savedInstanceState: Bundle?): Dialog =
                AlertDialog.Builder(activity)
                        .setMessage(requireArguments().getString(ARG_MESSAGE))
                        .setPositiveButton(android.R.string.ok) { _, _ -> requireActivity().finish() }
                        .create()

        override fun show(childFragmentManager: FragmentManager, tag: String?) {

        }

        companion object {

            @JvmStatic
            private val ARG_MESSAGE = "message"

            @JvmStatic
            fun newInstance(message: String): ErrorDialog = ErrorDialog().apply {
                arguments = Bundle().apply { putString(ARG_MESSAGE, message) }
            }
        }
    }

    companion object {
        /**
         * Conversion from screen rotation to JPEG orientation.
         */
        private val ORIENTATIONS = SparseIntArray()
        private val FRAGMENT_DIALOG = "dialog"

        init {
            ORIENTATIONS.append(Surface.ROTATION_0, 90)
            ORIENTATIONS.append(Surface.ROTATION_90, 0)
            ORIENTATIONS.append(Surface.ROTATION_180, 270)
            ORIENTATIONS.append(Surface.ROTATION_270, 180)
        }

        /**
         * Tag for the [Log].
         */
        private const val TAG = "PosenetActivity"
    }

    @SuppressLint("UseRequireInsteadOfGet")
    fun transfer_intant(targetFilename: String) {
        val intent = Intent(context, physicalResult::class.java).apply {
        }
        intent.putExtra("key", targetFilename)

        intent.putExtra("grade",grade)

        startActivity(intent)
    }

    fun BoundaryCheck(point:Point,check:Boolean):Int{
        Log.i("발",point.toString())
        if(point.y>210){
            if(point.x>150||point.x<100){
                if(check==true){
                    left_check=false
                    if(point.x>200){
                        grade+= 5
                    }
                }else{
                    left_check=true
                    if(point.x<100){
                        grade+= 5
                    }
                }
                return 1
            }
            else{
                grade-=3
                return 3
            }
        }
        return 2
    }
}